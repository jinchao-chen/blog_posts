{
  
    
        "post0": {
            "title": "How to Build a Personalized Trading Dashboard?",
            "content": "TL;NR: if you are mainly interested in the codes, here is the link to GitHub . Motivation . Yesterday I came across an article The Boredom Economy. Sydney Ember explained the GameStop phenomenon as investors&#39; reaction to the boredom experienced during the pandemic. . Being one amateur day traders new to the market, I fully concur with the explanation Sydney put forth. I noticed myself spending hours daily analyzing the market and trading frequently for profits, as an escape from boredom. Resultantly, I I generated an amount of data in 2020 that could pertentially be used for a study on my &#39;trading style&#39;. . With this in mind, I decided started a project on analysing my trading activities. To start with, I create a dashboard to visualize the activities, to understand how and when I tend to buy/sell a stock. . Preparation . My primary trading platform is Trading212. The platform recently included a new feature that allows exporting transaction history in csv format. The exported data is clean neatly structured, which is ready for analysis. . For the tools, I noticed a repo on GitHub panel-altair-dashboard that creates a simple, yet powerful, visualization tool (dashboard) in roughly 25 lines of codes. It is achieved by with Panel and Altair. . To achieve visualizing my trading activities, I include the following features: . mark transaction actions (sell or buy), in the stock time history | plot the stock historical data using candlestick | . Time to visualize the time series! . Below is a screenshot of the dashboard. In it, the transaction data (buy or sell)is visualzied along with the market data. There is an option to visualize the market data in either line plot or candlestick, depending on whether you are interested in long term trends or the variations within the day. For a demo, please refer to binder. . It is still at the very early stage of development. In the future, I would like to add the following, . provide a summary of my portfolio | normalize the stock price for the selected during | . And more features will be inlcuded, if I find anything interesting! . .",
            "url": "https://jinchao-chen.github.io/blog_posts/data%20analysis/trading/jupternotebook/2021/02/21/portfolio_analysis.html",
            "relUrl": "/data%20analysis/trading/jupternotebook/2021/02/21/portfolio_analysis.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Based Wetsuit Buying Guide",
            "content": "Introduction . I caught the surfing bug during a vacation in Portugal and has been an weekend warrior ever since. In contrast to the warm and pleasant subtropical conditions in Protugal, the Dutch surf scene is quite different: characterized by cold water, wind and rain. As one who surfs only during the weekends, I am truely grateful that my wetsuits allowes surf long sessions without worrying about being cold. . Recently, I decided to invest in some quality wetsuits, to surf all year round. As an engineer who rely on data to make decisions, I started this project analyzing the surf reports on surfweer. Most of the forecasts contain a section on wetsuit recommendations that forms the input of my project. . After web crawling and data cleaning, I was able to gather a number of records for the period from 2019 to 2020. With available information, I would like to get to know: . How often is there a &quot;surf day&quot;? | What types of wetsuit do I need to surf in different seasons? | . a = [2019, 2020] df = df[df[&quot;year&quot;].isin(a)].copy() df[&quot;month&quot;] = df[&quot;month&quot;].astype(&quot;str&quot;) df[&quot;month&quot;] = df[&quot;month&quot;].apply(lambda x: month_name(x)) . Monthly Surf Days . The collected data contains 239 reports, from January 2019 to February 2021. Before looking into the wetsuits, we will first have a look at the surf condition. From the chart, it is observed that swells are mostly active in early spring and autumn. The frequency dips during the summer months. Note that data was missing for April 2020, due to the pandemic. . Wetsuit Recommendations . According to srface, brands usually advertise their wetsuit neoprene thicknesses as 3/2, 4/3, 5/4, 6/4, etc. 3/2 for instance, means this wetsuit’s main panels are 3mm and 2mm thick. Normally, the chest and back panels are made out of thicker neoprene foam for extra warmth. Arms, shoulders, and legs are usually thinner for more flexibility. . I chose to use the main panel thickness only in the analysis. Wetsuit will be renamed using 3, 4, 5, 6. . Wetsuit . The bar below presents an overview of wetsuit requirement for each month. From the chart, it is shown that either 5mm or 6mm wetsuit is required to surf in the winter months (Nov to Mar). In spring and fall, it requires a combination of 4mm and 5mm wetsuit. A thin wetsuit of 3 mm is suitable for the summer months, especially July and August. . alt.Chart(df).mark_bar().encode( y=alt.Y(&quot;month:N&quot;, axis=alt.Axis(title=&quot;month&quot;), sort=month_names), x=alt.X(&quot;sum(report_count)&quot;, axis=alt.Axis(title=&quot;counts&quot;)), color=alt.Color(&quot;wetsuit:N&quot;), ).properties() . The chart underneath presents an overview of the number of days that you will need each type of wetsuit throughout the year: 5mm is the mostly often used wetsuit accounting for 40.5 days, which is about 30% of the total surfable days. 3mm wetsuit in the second place, and you could wear it for 33.5 days. . This interactive plot presents the wetsuit recommendation for each month, for each type of the wetsuit. By adjusting the thickness, you could get to know the months and total days that you could surf after purchasing that type of wetsuit. . Which to choose? . Based on the data above, I would like to provide recommendation for three scenarios: . Luxury: All of them. With this invested, you are ready for all weather conditoins! | Comfort: To maximize the comfort throughout the year, I find it necessary to buy a 5mm and a 3mm wetsuit as the minimum. With these two at hand, you are ready for 80% of surf days, if you subsitue 4mm with either a 5mm or a 3mm on the days when 4mm is recommended. | Budget: Let&#39; be realistic, wetsuit is expensive. Considering most of us have day job, we won&#39;t wear it that often throughout the year. If the budget is limited, I would recommend purchasing a 4mm wetsuit. With that at hand, you will still be able to surf 50% of the time, if you don&#39;t mind being in a 4mm in a tropical day. But it at least made you fully prepared for April, May, September and October which are the months with the best conditions in my opinion. | .",
            "url": "https://jinchao-chen.github.io/blog_posts/data%20analysis/surfing/jupternotebook/2021/02/14/wetsuit-guide.html",
            "relUrl": "/data%20analysis/surfing/jupternotebook/2021/02/14/wetsuit-guide.html",
            "date": " • Feb 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jinchao-chen.github.io/blog_posts/jupyter/2021/01/13/test.html",
            "relUrl": "/jupyter/2021/01/13/test.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Customer Segmentation for Arvato Financial Services",
            "content": "Project Definition . project overview . This blog post is part of the Capstone Project for the Udacity Data Scientist nano degree program. In this project, the demographics data was utilized, to identify the potential customers for the mail-order company, a client of Arvato Financial Services. The supervised machine learning model, the outcome of this project, predicts whether or not individuals will respond to the campaign. . The blog post is composed of three parts: . Data Exploration: Understanding the data and data cleaning . | Unsupervised learning: a customer segmentation report . | Supervised learning: a classifier that predicts whether an individual will respond . | . project statement . In the first part, the main focus is on cleaning the data and exploring feature engineering options. With it completed, the data is then used to build an unsupervised machine learning model that performs segmentation for the customers and the general population. This model works to group individuals based on their features. By comparing to the general population, we could identify features that best describe the customers. Lastly, a prediction model was created to identify individuals who are more likely to respond to the campaign. . metrics . The prediction model is a binary classifier that separates the target group (who will respond) from the non-target group. As the dataset is highly imbalanced with a very low positive rate (1.2%), accuracy alone won’t be the appropriate evaluation metric to measure how well the model classifies the minority group. Instead, we will use the ROC AUC score to evaluate model performance. When evaluating this binary classifier, we like to capture all the positive groups while minimizing the false positive rate. ROC AUC score is, therefore, an appropriate metric, since it summarizes the trade-off between the true-positive rate and false-positive rate. . PART I: Data preparation . There are four data files associated with this project: . Udacity_AZDIAS_052018.csv: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns). . | Udacity_CUSTOMERS_052018.csv: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns). . | Udacity_MAILOUT_052018_TRAIN.csv: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns). . | Udacity_MAILOUT_052018_TEST.csv: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns). . | . In addition to the above, Arvato provides two information files that describe attributes depicted in the data file. . DIAS Information Levels — Attributes 2017.xlsx: is a top-level list of attributes and descriptions, organized by informational category. . | DIAS Attributes — Values 2017.xlsx: is a detailed mapping of data values for each feature in alphabetical order. . | . Feature summary — data exploration . As the first step, I created a feature summary file that summarizes the names of the attributes, the datatype, and missing or unknown representation. This summary file forms the basis of data preprocessing. . Screenshot of feature summary . A quick scan shows that the data in the file is mostly ordinal. For example, “D19_BANKEN_ANZ_12” describes the frequency of transaction activity. The values range from least frequent (0 = very low activity) to the most frequent (6 = very high activity). Since it is ranking based, the datatype “D19_BANKEN_ANZ_12” is ordinal. As most of the features are ordinal, I will pre-assign the feature type as ‘ordinal’. If further analysis shows otherwise, I will update the summary table. . In addition, it is noticed that the naming of most features follows a fixed structure. For example, in “D19_NAHRUNGSERGAENZUNG_RZ”, the first part “D19” presents the data source, while the rest describe the content of the feature (NAHRUNGSERGAENZUN = FOOD SUPPLEMENT). As data from the same source tend to be encoded in a similar manner, I decided to split the attribute names and use the first part (prefix) to investigate feature encoding in groups. By grouping the features, I was able to create a summary table for 276 features in a structured way. . top-ranked prefixes used in naming . Furthermore, it is noticed from the analysis that: . Non-numeric values are assigned to a number of columns: “OST_WEST_KZ”, “CAMEO_DEUG_2015”, “CAMEO_DEU_2015”, “CAMEO_INTL_2015”, “D19_LETZTER_KAUF_BRANCHE”. I will therefore use LabelEncoder to transform it, and replace the abnormal values (“X”, “XX”) as unknowns. . | ‘EINGEFUEGT_AM’ uses date and time as feature values. I extracted the year from the string, which will be treated as numerical data. . | As presented in the bar chart below, most of the features are ordinal, with few features are numerical. The rest, 73 features, is treated as categorical. . | . Distribution of datatypes among the features . Data cleaning, transformation, and feature engineering . The data cleaning involves multiple steps: . Convert missing values to NaNs. . | Drop rows and columns where a large portion are NaN’s . | Replacing Missing values with the most frequent. . | Outlier detection using Z-score . | One Hot Encoding for Categorical Data. . | Min-max scaling for ordinal and numerical data . | Create a data cleaning pipeline, to be used later for later analysis . | . The data pre-processing pipeline includes two steps: . the first step ‘reduce dimension’ includes mapping the unknowns, selecting columns of interests, remove . | the second step ‘combined pipe’ applies different data transformation strategies according to the datatypes. . | . At the first step, I decided to keep all the features in the analysis: the nan-threshold appears to cause data drain, which hampers the performance of the supervised machine learning model. . At the second step, the nan’s were replaced with either the median or the most frequent value. Afterward, the numerical values were scaled using MinMax scaler while the categorical values were transformed using OneHotEncoder. . PART II: Customer segmentation report . In this section, unsupervised learning techniques are utilized to compare the demographic data of the customers and the general population. . Principal Component Analysis . To start with, I used Principal Component Analysis (PCA) to summarize the information contained in the data files. This is an efficient way to combine similar features while maintaining the variance of the data. . . The accumulated explained variance increases, with the increase in the number of principal components included in the model. It was decided to include the first 216 PCA’s, which captures more than 80% of the accumulated variance. . Clustering . The clustering is based on the elbow method. I decided to include 10 clusters in total since additional clusters only slightly reduce the inertia of KNMeans. . . Using the clustering model, I compared the difference between the customers and the general population. The figure below presents the distribution of the data over the 10 clusters. From the figures, it is concluded that customers tend to belong to cluster #2 and cluster #7, while the general population is evenly distributed among the clusters. . . . The bar chart above presents the difference between the general population and the customers, in terms of the proportion of each cluster. It is noticed that differences are the largest in cluster 5 and cluster 7. . . The heatmap above presents the relevance of each PCA to each cluster. Combined with the comparison between the general population and customer, we could infer that potential customers tend to live in a neighborhood with a higher number of car ownership, and they are older than the general population. . PART III: Prediction . With segmentation completed, a supervised machine learning model was built to predict whether the individuals in the dataset will convert to be a customer. The analysis is based on demographics contained Udacity_MAILOUT_052018_TRAIN.csv (trainig data)and Udacity_MAILOUT_052018_TEST (test dta).csv. The training data contains one additional column named ‘response’ while we need to predict it for the testing dataset. . Training set: total records: 42962 who responded to mail out campain: 532 who did not respond: 42430 na&#39;s in the RESPONSE column: 0 portion of response: 1.24% . A further examination shows that only 1.24% of people responded to the campaign in the training set, which results in a highly imbalanced data set. To deal with the imbalance dataset in this binary classification, I used the ROC AUC score to evaluate model performance that measures both the true positive and false-positive rates. . In modeling, I used Ensembling classifiers as it shows improved performances over a single learner. Besides, I made use of the balanced ensemble methods provided in [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html), which internally samples the data. As concluded in ref.[1], the balanced Ensembling classifier greatly enhances model performance. . Confusion matrixes, using different classifiers . From the estimators that I compared, EasyEnsemble *was selected as it yielded the highest ROC AUC score. It yielded a ROC AUC score of 0.78, even without hyperparameter tuning. The non-balanced models (Bagging, AdaBoost) have a score of 0.5, which indicates the models failed to provide a prediction and all were predicted to be negative. In comparison, the balanced models show significantly improved performance, thanks to the balancing techniques internally implemented in *imbalanced-learn. . Feature importance . Figure about shows the feature importance. ‘D19_SOZIALES’ has the highest weight in the prediction model. A potential customer tends to have a value of ‘1.0 ’ in ‘D19_SOZIALES’. Unfortunately, the meaning of this feature was not provided in the information file. We were unable to further interpret the feature. . Influence of the total number of features in ROC AUC score. . It however strikes me that the performance of the model is not highly dependent on the number of features used to create in the model. It already yields a ROC AUC score of 0.78, when including the top three of the most important features. . Further efforts were spared in fine-tuning the model, as provided below. From the parameters compared, it is decided to include 30 estimators for the base model and 30 estimators for the *EasyEnsembleClassifier. *The ROC AUC score is slightly increased to 0.80. . Conclusions . From the confusion matrix for EasyEnsemble, it is noted that the model has a true positive rate of 0.88. The classifier manages to identify the majority of the individuals who will respond. Even if we limit the campaign to the individuals who are predicted to be positive, we could still reach 88% of the target group (true positives). If adopting this strategy in the campaign, we could effectively reduce the campaign to 33% of its original size. In conclusion, the machine learning model works to predict whether or not each individual will respond to the campaign. The has the potential to help the mail-order company to better target the customers. . This is the first machine learning model that I created using real-world data. It is quite challenging a project in tow aspects: . The large dimension of the data set requires extensive efforts in understanding the data and establishing a strategy to process the data . | It is also highly imbalanced, which requires proper sampling to be suitable for classification analysis. . | . The deeper that I dived into the data, the more I realized the importance of proper feature engineering. To enhance the performance of the data, I would refine the feature engineering strategies, to better select and extract information from the data set. Besides, I would also like to explore more advanced classifiers with enhanced performances in dealing with an imbalanced data set. . Reference: . [1].Comparison of ensembling classifiers internally using sampling: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/ensemble/plot_comparison_ensemble_classifier.html#sphx-glr-auto-examples-ensemble-plot-comparison-ensemble-classifier-py .",
            "url": "https://jinchao-chen.github.io/blog_posts/markdown/2020/12/08/Arvato_Financial.html",
            "relUrl": "/markdown/2020/12/08/Arvato_Financial.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "How Did Road Freight Transport Develop in the Netherlands?",
            "content": "I spent hours daily driving on one of the busiest motorways in the Netherlands when commuting was still a norm. When I first came across with the goods vehicle data on CBS website, it immediately attracted my attention: it could answer those not so important questions that I have always had, when stuck in a traffic jam after a long day of work, alongside the lorries. . In this article, we will explore several aspects of goods vehicles transportation in particular, . What are the types of common goods vehicles? How can we tell them apart using data? . | What is the lifespan of a goods vehicle? . | Why is there such a high volume of goods vehicles on the motorways nowadays? . | . General information . As described on CBS website, the data provides information . … on total vehicle kilometres of goods vehicles in the Netherlands (broken down by Dutch and foreign vehicles) and data of total kilometres and average annual kilometres of Dutch goods vehicles (broken down by Dutch and foreign territory). All figures are further broken down by lorries and road tractors, by age of the vehicle and by load capacity. . In the analysis, we will answer the question using mainly the following: . country of origin: Dutch or foreign vehicles . | vehicle type: lorries or road tractors . | vehicle age: deducted from the year of construction . | vehicle km territory: abroad or in the Netherlands . | . Goods vehicle type . The original dataset breaks the goods vehicle into several groups and subgroups. For simplicity concerns, we will treat them as either lorry or road tractor. . Amount of Dutch goods vehicles in use, from 2001 to 2018 . Kilometres by different vehicles types, from 2001 to 2018 . From the charts above, it can be concluded that road tractor is gradually gaining popularity as of 2001. Road tractor total kilometres outnumber lorries in 2002. The registered amount of road tractor outnumbered lorries in 2008. In 2018, road tractor kilometres was already more than twice that of the lorry. . . What about the annual average? The figure above compares the average annual kilometres in and outside the Netherlands, for lorries and road tractors. From the figure above, it is observed that: . Road tractors are more involved in extra-territory transportation than lorries; . | Road tractors tend to have larger annual kilometres than the lorries, both in and outside the Netherlands. . | . The lifespan of goods vehicle . . What is the service life of goods vehicles? Instead of directly answering it, we will tweak this question slightly and investigate how long it takes for half of the newly constructed to become de-registered. . The figure above presents the percentage of the goods vehicle that remains in use, to the number of years in service. A clear distinction is present between the analyzed vehicle types: 50% of the newly constructed road tractors become de-registered after being in use for 9 years; for lorries, that occurs 3 years later when they are in use for 12 years. . Why so many goods vehicles nowadays? . I always hear “it is a sign of booming economy” when talking to my Dutch fellows about the over crowded motorways. With this in mind, I looked into CBS data, in combination with GDP records from the World Bank. . . The chart above presents GPD (current USD)and goods vehicle km over the period from 2001 to 2018. A few observations can be made: . Vehicle km is highly correlated to economic growth: it increases during the economic boom and shrinks in the bust. . | Road transport seems to lead the cycle. While freight transport recovery started in 2014, the economy entered the growth cycle one year later in 2015. Similarly, the traffic kilometers reached its peak one year earlier in 2017 while the economy peaked in 2018. . | . . A similar trend can also be observed in the number of newly constructed or registered vehicles. During the years of 2009 and 2010, there is a significant reduction in the number of near construction. Investment in new vehicles started to recover from 2011 and returned to the pre-crisis level in 2015. . What I learned from the data . A few items can be learned from the exploratory data analysis, . The data contains information about two types of goods vehicle: lorries and road tractors; . | Road tractor is gradually gaining popularity in recent years, with more road tractor entering the market than lorries; . | Road tractors are more involved in extra-territory transportation than lorries while lorries tend to travel more often within the country. . | Road tractors have larger annual kilometres than the lorries, both in and outside the Netherlands; . | With regards to the life span, 50% of road tractors becomes de-registered after 9 years while for lorries that occurs after being in service for 12 years; . | It is concluded that freight transport is highly dependent on the economy. The variation of the vehicle kilometers closely follows the economic cycle. . | . That is it for now. Next step: build a regression model to classify vehicle types. . All the used scripts are available in the git repo. . Reference . Vehicle kilometres of lorries and road tractors (Dutch and foreign vehicles) by territory, vehicle construction years, gross vehicle weight, CBS . GDP data: https://data.worldbank.org/country/netherlands .",
            "url": "https://jinchao-chen.github.io/blog_posts/data%20analysis/markdown/2020/08/26/Freight_Transport.html",
            "relUrl": "/data%20analysis/markdown/2020/08/26/Freight_Transport.html",
            "date": " • Aug 26, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jinchao-chen.github.io/blog_posts/markdown/2020/01/14/test-markdown-post-hided.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post-hided.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Jinchao Chen . :cn: Chinese on an adventure in Europe :man_mechanic: An engineer who is better at coding than drafting :surfing_man: A thalassophile in addiction with ocean and waves .",
          "url": "https://jinchao-chen.github.io/blog_posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jinchao-chen.github.io/blog_posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}