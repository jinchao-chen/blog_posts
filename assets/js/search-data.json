{
  
    
        "post0": {
            "title": "Data Based Wetsuit Buying Guide",
            "content": "Introduction . I got the surfing bug during a vacation in Portugal and has been an weekend warrior in Scheveningen ever since. In contrast to the warm and pleasant subtropical conditions in Protugal, the Dutch surf scene is quite different characterized by wind and rains. As one who surfs only during the weekends, I am truely grateful that my wetsuits allowes surf til fatigue without worrying about being cold. . Recently, I decided to invest in some quality wetsuits, in order to surf all year round. As an engineer who always rely on data to make decisions, I started this project to analyze the surf reports posted on surfweer. Most of the forecasts contain a section on wetsuit recommendations that forms the input of my project. . After web crawling and data cleaning, I was able to gather a few entires that covers the period from 2019 to 2020. With these available information, I would like to get to know: . How often is there a surfable condition? | What types of wetsuit do I need to surf in different seasons? | . a = [2019, 2020] df = df[df[&quot;year&quot;].isin(a)].copy() . Monthly Surf Days . The data contains . . Observations: . swells are mostly active in spring and autumn. the frequency peaks in | camel peaks distribution | . Wetsuit Recommendations . As per srface, brands usually advertise their wetsuit neoprene thicknesses as 3/2, 4/3, 5/4, 6/4, etc. 3/2 for instance, means this wetsuit’s main panels are 3mm and 2mm thick. Normally, the chest and back panels are made out of thicker neoprene foam for extra warmth. Arms, shoulders, and legs are usually thinner for more flexibility. . I chose to use the main panel thickness only in the analysis, to reduce the number of types. It will be renamed using 3, 4, 5, 6. . Wetsuit . monthly = df.groupby([&quot;wetsuit&quot;])[&quot;report_date&quot;].count() monthly = monthly.reset_index() . . ax = sns.barplot(x=&quot;wetsuit&quot;, y=&quot;report_date&quot;, data=monthly, width=.5) ax.set_xlabel(&quot;wetsuit thickness&quot;) ax.set_ylabel(&quot;days&quot;) # ax.set_xticks(np.arange(1, 12 + 1, 1.0)) ax.set_xticklabels([&quot;3 mm&quot;, &quot;4 mm&quot;, &quot;5 mm&quot;, &quot;6 mm&quot;]); . TypeError Traceback (most recent call last) &lt;ipython-input-22-4455fe13e942&gt; in &lt;module&gt; 1 # hid_input -&gt; 2 ax = sns.barplot(x=&#34;wetsuit&#34;, y=&#34;report_date&#34;, data=monthly, width=.5) 3 4 ax.set_xlabel(&#34;wetsuit thickness&#34;) 5 ax.set_ylabel(&#34;days&#34;) ~/opt/miniconda3/lib/python3.8/site-packages/seaborn/_decorators.py in inner_f(*args, **kwargs) 44 ) 45 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}) &gt; 46 return f(**kwargs) 47 return inner_f 48 ~/opt/miniconda3/lib/python3.8/site-packages/seaborn/categorical.py in barplot(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs) 3185 ax = plt.gca() 3186 -&gt; 3187 plotter.plot(ax, kwargs) 3188 return ax 3189 ~/opt/miniconda3/lib/python3.8/site-packages/seaborn/categorical.py in plot(self, ax, bar_kws) 1637 def plot(self, ax, bar_kws): 1638 &#34;&#34;&#34;Make the plot.&#34;&#34;&#34; -&gt; 1639 self.draw_bars(ax, bar_kws) 1640 self.annotate_axes(ax) 1641 if self.orient == &#34;h&#34;: ~/opt/miniconda3/lib/python3.8/site-packages/seaborn/categorical.py in draw_bars(self, ax, kws) 1602 1603 # Draw the bars -&gt; 1604 barfunc(barpos, self.statistic, self.width, 1605 color=self.colors, align=&#34;center&#34;, **kws) 1606 ~/opt/miniconda3/lib/python3.8/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs) 1436 def inner(ax, *args, data=None, **kwargs): 1437 if data is None: -&gt; 1438 return func(ax, *map(sanitize_sequence, args), **kwargs) 1439 1440 bound = new_sig.bind(ax, *args, **kwargs) TypeError: bar() got multiple values for argument &#39;width&#39; . sns.barplot?? . Signature: sns.barplot( *, x=None, y=None, hue=None, data=None, order=None, hue_order=None, estimator=&lt;function mean at 0x7fbefdda3ee0&gt;, ci=95, n_boot=1000, units=None, seed=None, orient=None, color=None, palette=None, saturation=0.75, errcolor=&#39;.26&#39;, errwidth=None, capsize=None, dodge=True, ax=None, **kwargs, ) Docstring: Show point estimates and confidence intervals as rectangular bars. A bar plot represents an estimate of central tendency for a numeric variable with the height of each rectangle and provides some indication of the uncertainty around that estimate using error bars. Bar plots include 0 in the quantitative axis range, and they are a good choice when 0 is a meaningful value for the quantitative variable, and you want to make comparisons against it. For datasets where 0 is not a meaningful value, a point plot will allow you to focus on differences between levels of one or more categorical variables. It is also important to keep in mind that a bar plot shows only the mean (or other estimator) value, but in many cases it may be more informative to show the distribution of values at each level of the categorical variables. In that case, other approaches such as a box or violin plot may be more appropriate. Input data can be passed in a variety of formats, including: - Vectors of data represented as lists, numpy arrays, or pandas Series objects passed directly to the ``x``, ``y``, and/or ``hue`` parameters. - A &#34;long-form&#34; DataFrame, in which case the ``x``, ``y``, and ``hue`` variables will determine how the data are plotted. - A &#34;wide-form&#34; DataFrame, such that each numeric column will be plotted. - An array or list of vectors. In most cases, it is possible to use numpy or Python objects, but pandas objects are preferable because the associated names will be used to annotate the axes. Additionally, you can use Categorical types for the grouping variables to control the order of plot elements. This function always treats one of the variables as categorical and draws data at ordinal positions (0, 1, ... n) on the relevant axis, even when the data has a numeric or date type. See the :ref:`tutorial &lt;categorical_tutorial&gt;` for more information. Parameters - x, y, hue : names of variables in ``data`` or vector data, optional Inputs for plotting long-form data. See examples for interpretation. data : DataFrame, array, or list of arrays, optional Dataset for plotting. If ``x`` and ``y`` are absent, this is interpreted as wide-form. Otherwise it is expected to be long-form. order, hue_order : lists of strings, optional Order to plot the categorical levels in, otherwise the levels are inferred from the data objects. estimator : callable that maps vector -&gt; scalar, optional Statistical function to estimate within each categorical bin. ci : float or &#34;sd&#34; or None, optional Size of confidence intervals to draw around estimated values. If &#34;sd&#34;, skip bootstrapping and draw the standard deviation of the observations. If ``None``, no bootstrapping will be performed, and error bars will not be drawn. n_boot : int, optional Number of bootstrap iterations to use when computing confidence intervals. units : name of variable in ``data`` or vector data, optional Identifier of sampling units, which will be used to perform a multilevel bootstrap and account for repeated measures design. seed : int, numpy.random.Generator, or numpy.random.RandomState, optional Seed or random number generator for reproducible bootstrapping. orient : &#34;v&#34; | &#34;h&#34;, optional Orientation of the plot (vertical or horizontal). This is usually inferred based on the type of the input variables, but it can be used to resolve ambiguitiy when both `x` and `y` are numeric or when plotting wide-form data. color : matplotlib color, optional Color for all of the elements, or seed for a gradient palette. palette : palette name, list, or dict Colors to use for the different levels of the ``hue`` variable. Should be something that can be interpreted by :func:`color_palette`, or a dictionary mapping hue levels to matplotlib colors. saturation : float, optional Proportion of the original saturation to draw colors at. Large patches often look better with slightly desaturated colors, but set this to ``1`` if you want the plot colors to perfectly match the input color spec. errcolor : matplotlib color Color for the lines that represent the confidence interval. errwidth : float, optional Thickness of error bar lines (and caps). capsize : float, optional Width of the &#34;caps&#34; on error bars. dodge : bool, optional When hue nesting is used, whether elements should be shifted along the categorical axis. ax : matplotlib Axes, optional Axes object to draw the plot onto, otherwise uses the current Axes. kwargs : key, value mappings Other keyword arguments are passed through to :meth:`matplotlib.axes.Axes.bar`. Returns - ax : matplotlib Axes Returns the Axes object with the plot drawn onto it. See Also -- countplot : Show the counts of observations in each categorical bin. pointplot : Show point estimates and confidence intervals using scatterplot glyphs. catplot : Combine a categorical plot with a :class:`FacetGrid`. Examples -- Draw a set of vertical bar plots grouped by a categorical variable: .. plot:: :context: close-figs &gt;&gt;&gt; import seaborn as sns &gt;&gt;&gt; sns.set_theme(style=&#34;whitegrid&#34;) &gt;&gt;&gt; tips = sns.load_dataset(&#34;tips&#34;) &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;total_bill&#34;, data=tips) Draw a set of vertical bars with nested grouping by a two variables: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;total_bill&#34;, hue=&#34;sex&#34;, data=tips) Draw a set of horizontal bars: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;tip&#34;, y=&#34;day&#34;, data=tips) Control bar order by passing an explicit order: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;time&#34;, y=&#34;tip&#34;, data=tips, ... order=[&#34;Dinner&#34;, &#34;Lunch&#34;]) Use median as the estimate of central tendency: .. plot:: :context: close-figs &gt;&gt;&gt; from numpy import median &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;tip&#34;, data=tips, estimator=median) Show the standard error of the mean with the error bars: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;tip&#34;, data=tips, ci=68) Show standard deviation of observations instead of a confidence interval: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;tip&#34;, data=tips, ci=&#34;sd&#34;) Add &#34;caps&#34; to the error bars: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;tip&#34;, data=tips, capsize=.2) Use a different color palette for the bars: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;size&#34;, y=&#34;total_bill&#34;, data=tips, ... palette=&#34;Blues_d&#34;) Use ``hue`` without changing bar position or width: .. plot:: :context: close-figs &gt;&gt;&gt; tips[&#34;weekend&#34;] = tips[&#34;day&#34;].isin([&#34;Sat&#34;, &#34;Sun&#34;]) &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;total_bill&#34;, hue=&#34;weekend&#34;, ... data=tips, dodge=False) Plot all bars in a single color: .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;size&#34;, y=&#34;total_bill&#34;, data=tips, ... color=&#34;salmon&#34;, saturation=.5) Use :meth:`matplotlib.axes.Axes.bar` parameters to control the style. .. plot:: :context: close-figs &gt;&gt;&gt; ax = sns.barplot(x=&#34;day&#34;, y=&#34;total_bill&#34;, data=tips, ... linewidth=2.5, facecolor=(1, 1, 1, 0), ... errcolor=&#34;.2&#34;, edgecolor=&#34;.2&#34;) Use :func:`catplot` to combine a :func:`barplot` and a :class:`FacetGrid`. This allows grouping within additional categorical variables. Using :func:`catplot` is safer than using :class:`FacetGrid` directly, as it ensures synchronization of variable order across facets: .. plot:: :context: close-figs &gt;&gt;&gt; g = sns.catplot(x=&#34;sex&#34;, y=&#34;total_bill&#34;, ... hue=&#34;smoker&#34;, col=&#34;time&#34;, ... data=tips, kind=&#34;bar&#34;, ... height=4, aspect=.7); Source: @_deprecate_positional_args def barplot( *, x=None, y=None, hue=None, data=None, order=None, hue_order=None, estimator=np.mean, ci=95, n_boot=1000, units=None, seed=None, orient=None, color=None, palette=None, saturation=.75, errcolor=&#34;.26&#34;, errwidth=None, capsize=None, dodge=True, ax=None, **kwargs, ): plotter = _BarPlotter(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge) if ax is None: ax = plt.gca() plotter.plot(ax, kwargs) return ax File: ~/opt/miniconda3/lib/python3.8/site-packages/seaborn/categorical.py Type: function . Schoen . Cap . .",
            "url": "https://jinchao-chen.github.io/blog_posts/data%20analysis/surfing/jupternotebook/2021/02/14/wetsuit-guide.html",
            "relUrl": "/data%20analysis/surfing/jupternotebook/2021/02/14/wetsuit-guide.html",
            "date": " • Feb 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jinchao-chen.github.io/blog_posts/jupyter/2021/01/13/test.html",
            "relUrl": "/jupyter/2021/01/13/test.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Customer Segmentation for Arvato Financial Services",
            "content": "Project Definition . project overview . This blog post is part of the Capstone Project for the Udacity Data Scientist nano degree program. In this project, the demographics data was utilized, to identify the potential customers for the mail-order company, a client of Arvato Financial Services. The supervised machine learning model, the outcome of this project, predicts whether or not individuals will respond to the campaign. . The blog post is composed of three parts: . Data Exploration: Understanding the data and data cleaning . | Unsupervised learning: a customer segmentation report . | Supervised learning: a classifier that predicts whether an individual will respond . | . project statement . In the first part, the main focus is on cleaning the data and exploring feature engineering options. With it completed, the data is then used to build an unsupervised machine learning model that performs segmentation for the customers and the general population. This model works to group individuals based on their features. By comparing to the general population, we could identify features that best describe the customers. Lastly, a prediction model was created to identify individuals who are more likely to respond to the campaign. . metrics . The prediction model is a binary classifier that separates the target group (who will respond) from the non-target group. As the dataset is highly imbalanced with a very low positive rate (1.2%), accuracy alone won’t be the appropriate evaluation metric to measure how well the model classifies the minority group. Instead, we will use the ROC AUC score to evaluate model performance. When evaluating this binary classifier, we like to capture all the positive groups while minimizing the false positive rate. ROC AUC score is, therefore, an appropriate metric, since it summarizes the trade-off between the true-positive rate and false-positive rate. . PART I: Data preparation . There are four data files associated with this project: . Udacity_AZDIAS_052018.csv: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns). . | Udacity_CUSTOMERS_052018.csv: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns). . | Udacity_MAILOUT_052018_TRAIN.csv: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns). . | Udacity_MAILOUT_052018_TEST.csv: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns). . | . In addition to the above, Arvato provides two information files that describe attributes depicted in the data file. . DIAS Information Levels — Attributes 2017.xlsx: is a top-level list of attributes and descriptions, organized by informational category. . | DIAS Attributes — Values 2017.xlsx: is a detailed mapping of data values for each feature in alphabetical order. . | . Feature summary — data exploration . As the first step, I created a feature summary file that summarizes the names of the attributes, the datatype, and missing or unknown representation. This summary file forms the basis of data preprocessing. . Screenshot of feature summary . A quick scan shows that the data in the file is mostly ordinal. For example, “D19_BANKEN_ANZ_12” describes the frequency of transaction activity. The values range from least frequent (0 = very low activity) to the most frequent (6 = very high activity). Since it is ranking based, the datatype “D19_BANKEN_ANZ_12” is ordinal. As most of the features are ordinal, I will pre-assign the feature type as ‘ordinal’. If further analysis shows otherwise, I will update the summary table. . In addition, it is noticed that the naming of most features follows a fixed structure. For example, in “D19_NAHRUNGSERGAENZUNG_RZ”, the first part “D19” presents the data source, while the rest describe the content of the feature (NAHRUNGSERGAENZUN = FOOD SUPPLEMENT). As data from the same source tend to be encoded in a similar manner, I decided to split the attribute names and use the first part (prefix) to investigate feature encoding in groups. By grouping the features, I was able to create a summary table for 276 features in a structured way. . top-ranked prefixes used in naming . Furthermore, it is noticed from the analysis that: . Non-numeric values are assigned to a number of columns: “OST_WEST_KZ”, “CAMEO_DEUG_2015”, “CAMEO_DEU_2015”, “CAMEO_INTL_2015”, “D19_LETZTER_KAUF_BRANCHE”. I will therefore use LabelEncoder to transform it, and replace the abnormal values (“X”, “XX”) as unknowns. . | ‘EINGEFUEGT_AM’ uses date and time as feature values. I extracted the year from the string, which will be treated as numerical data. . | As presented in the bar chart below, most of the features are ordinal, with few features are numerical. The rest, 73 features, is treated as categorical. . | . Distribution of datatypes among the features . Data cleaning, transformation, and feature engineering . The data cleaning involves multiple steps: . Convert missing values to NaNs. . | Drop rows and columns where a large portion are NaN’s . | Replacing Missing values with the most frequent. . | Outlier detection using Z-score . | One Hot Encoding for Categorical Data. . | Min-max scaling for ordinal and numerical data . | Create a data cleaning pipeline, to be used later for later analysis . | . The data pre-processing pipeline includes two steps: . the first step ‘reduce dimension’ includes mapping the unknowns, selecting columns of interests, remove . | the second step ‘combined pipe’ applies different data transformation strategies according to the datatypes. . | . At the first step, I decided to keep all the features in the analysis: the nan-threshold appears to cause data drain, which hampers the performance of the supervised machine learning model. . At the second step, the nan’s were replaced with either the median or the most frequent value. Afterward, the numerical values were scaled using MinMax scaler while the categorical values were transformed using OneHotEncoder. . PART II: Customer segmentation report . In this section, unsupervised learning techniques are utilized to compare the demographic data of the customers and the general population. . Principal Component Analysis . To start with, I used Principal Component Analysis (PCA) to summarize the information contained in the data files. This is an efficient way to combine similar features while maintaining the variance of the data. . . The accumulated explained variance increases, with the increase in the number of principal components included in the model. It was decided to include the first 216 PCA’s, which captures more than 80% of the accumulated variance. . Clustering . The clustering is based on the elbow method. I decided to include 10 clusters in total since additional clusters only slightly reduce the inertia of KNMeans. . . Using the clustering model, I compared the difference between the customers and the general population. The figure below presents the distribution of the data over the 10 clusters. From the figures, it is concluded that customers tend to belong to cluster #2 and cluster #7, while the general population is evenly distributed among the clusters. . . . The bar chart above presents the difference between the general population and the customers, in terms of the proportion of each cluster. It is noticed that differences are the largest in cluster 5 and cluster 7. . . The heatmap above presents the relevance of each PCA to each cluster. Combined with the comparison between the general population and customer, we could infer that potential customers tend to live in a neighborhood with a higher number of car ownership, and they are older than the general population. . PART III: Prediction . With segmentation completed, a supervised machine learning model was built to predict whether the individuals in the dataset will convert to be a customer. The analysis is based on demographics contained Udacity_MAILOUT_052018_TRAIN.csv (trainig data)and Udacity_MAILOUT_052018_TEST (test dta).csv. The training data contains one additional column named ‘response’ while we need to predict it for the testing dataset. . Training set: total records: 42962 who responded to mail out campain: 532 who did not respond: 42430 na&#39;s in the RESPONSE column: 0 portion of response: 1.24% . A further examination shows that only 1.24% of people responded to the campaign in the training set, which results in a highly imbalanced data set. To deal with the imbalance dataset in this binary classification, I used the ROC AUC score to evaluate model performance that measures both the true positive and false-positive rates. . In modeling, I used Ensembling classifiers as it shows improved performances over a single learner. Besides, I made use of the balanced ensemble methods provided in [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html), which internally samples the data. As concluded in ref.[1], the balanced Ensembling classifier greatly enhances model performance. . Confusion matrixes, using different classifiers . From the estimators that I compared, EasyEnsemble *was selected as it yielded the highest ROC AUC score. It yielded a ROC AUC score of 0.78, even without hyperparameter tuning. The non-balanced models (Bagging, AdaBoost) have a score of 0.5, which indicates the models failed to provide a prediction and all were predicted to be negative. In comparison, the balanced models show significantly improved performance, thanks to the balancing techniques internally implemented in *imbalanced-learn. . Feature importance . Figure about shows the feature importance. ‘D19_SOZIALES’ has the highest weight in the prediction model. A potential customer tends to have a value of ‘1.0 ’ in ‘D19_SOZIALES’. Unfortunately, the meaning of this feature was not provided in the information file. We were unable to further interpret the feature. . Influence of the total number of features in ROC AUC score. . It however strikes me that the performance of the model is not highly dependent on the number of features used to create in the model. It already yields a ROC AUC score of 0.78, when including the top three of the most important features. . Further efforts were spared in fine-tuning the model, as provided below. From the parameters compared, it is decided to include 30 estimators for the base model and 30 estimators for the *EasyEnsembleClassifier. *The ROC AUC score is slightly increased to 0.80. . Conclusions . From the confusion matrix for EasyEnsemble, it is noted that the model has a true positive rate of 0.88. The classifier manages to identify the majority of the individuals who will respond. Even if we limit the campaign to the individuals who are predicted to be positive, we could still reach 88% of the target group (true positives). If adopting this strategy in the campaign, we could effectively reduce the campaign to 33% of its original size. In conclusion, the machine learning model works to predict whether or not each individual will respond to the campaign. The has the potential to help the mail-order company to better target the customers. . This is the first machine learning model that I created using real-world data. It is quite challenging a project in tow aspects: . The large dimension of the data set requires extensive efforts in understanding the data and establishing a strategy to process the data . | It is also highly imbalanced, which requires proper sampling to be suitable for classification analysis. . | . The deeper that I dived into the data, the more I realized the importance of proper feature engineering. To enhance the performance of the data, I would refine the feature engineering strategies, to better select and extract information from the data set. Besides, I would also like to explore more advanced classifiers with enhanced performances in dealing with an imbalanced data set. . Reference: . [1].Comparison of ensembling classifiers internally using sampling: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/ensemble/plot_comparison_ensemble_classifier.html#sphx-glr-auto-examples-ensemble-plot-comparison-ensemble-classifier-py .",
            "url": "https://jinchao-chen.github.io/blog_posts/markdown/2020/12/08/Arvato_Financial.html",
            "relUrl": "/markdown/2020/12/08/Arvato_Financial.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "How Did Road Freight Transport Develop in the Netherlands?",
            "content": "I spent hours daily driving on one of the busiest motorways in the Netherlands when commuting was still a norm. When I first came across with the goods vehicle data on CBS website, it immediately attracted my attention: it could answer those not so important questions that I have always had, when stuck in a traffic jam after a long day of work, alongside the lorries. . In this article, we will explore several aspects of goods vehicles transportation in particular, . What are the types of common goods vehicles? How can we tell them apart using data? . | What is the lifespan of a goods vehicle? . | Why is there such a high volume of goods vehicles on the motorways nowadays? . | . General information . As described on CBS website, the data provides information . … on total vehicle kilometres of goods vehicles in the Netherlands (broken down by Dutch and foreign vehicles) and data of total kilometres and average annual kilometres of Dutch goods vehicles (broken down by Dutch and foreign territory). All figures are further broken down by lorries and road tractors, by age of the vehicle and by load capacity. . In the analysis, we will answer the question using mainly the following: . country of origin: Dutch or foreign vehicles . | vehicle type: lorries or road tractors . | vehicle age: deducted from the year of construction . | vehicle km territory: abroad or in the Netherlands . | . Goods vehicle type . The original dataset breaks the goods vehicle into several groups and subgroups. For simplicity concerns, we will treat them as either lorry or road tractor. . Amount of Dutch goods vehicles in use, from 2001 to 2018 . Kilometres by different vehicles types, from 2001 to 2018 . From the charts above, it can be concluded that road tractor is gradually gaining popularity as of 2001. Road tractor total kilometres outnumber lorries in 2002. The registered amount of road tractor outnumbered lorries in 2008. In 2018, road tractor kilometres was already more than twice that of the lorry. . . What about the annual average? The figure above compares the average annual kilometres in and outside the Netherlands, for lorries and road tractors. From the figure above, it is observed that: . Road tractors are more involved in extra-territory transportation than lorries; . | Road tractors tend to have larger annual kilometres than the lorries, both in and outside the Netherlands. . | . The lifespan of goods vehicle . . What is the service life of goods vehicles? Instead of directly answering it, we will tweak this question slightly and investigate how long it takes for half of the newly constructed to become de-registered. . The figure above presents the percentage of the goods vehicle that remains in use, to the number of years in service. A clear distinction is present between the analyzed vehicle types: 50% of the newly constructed road tractors become de-registered after being in use for 9 years; for lorries, that occurs 3 years later when they are in use for 12 years. . Why so many goods vehicles nowadays? . I always hear “it is a sign of booming economy” when talking to my Dutch fellows about the over crowded motorways. With this in mind, I looked into CBS data, in combination with GDP records from the World Bank. . . The chart above presents GPD (current USD)and goods vehicle km over the period from 2001 to 2018. A few observations can be made: . Vehicle km is highly correlated to economic growth: it increases during the economic boom and shrinks in the bust. . | Road transport seems to lead the cycle. While freight transport recovery started in 2014, the economy entered the growth cycle one year later in 2015. Similarly, the traffic kilometers reached its peak one year earlier in 2017 while the economy peaked in 2018. . | . . A similar trend can also be observed in the number of newly constructed or registered vehicles. During the years of 2009 and 2010, there is a significant reduction in the number of near construction. Investment in new vehicles started to recover from 2011 and returned to the pre-crisis level in 2015. . What I learned from the data . A few items can be learned from the exploratory data analysis, . The data contains information about two types of goods vehicle: lorries and road tractors; . | Road tractor is gradually gaining popularity in recent years, with more road tractor entering the market than lorries; . | Road tractors are more involved in extra-territory transportation than lorries while lorries tend to travel more often within the country. . | Road tractors have larger annual kilometres than the lorries, both in and outside the Netherlands; . | With regards to the life span, 50% of road tractors becomes de-registered after 9 years while for lorries that occurs after being in service for 12 years; . | It is concluded that freight transport is highly dependent on the economy. The variation of the vehicle kilometers closely follows the economic cycle. . | . That is it for now. Next step: build a regression model to classify vehicle types. . All the used scripts are available in the git repo. . Reference . Vehicle kilometres of lorries and road tractors (Dutch and foreign vehicles) by territory, vehicle construction years, gross vehicle weight, CBS . GDP data: https://data.worldbank.org/country/netherlands .",
            "url": "https://jinchao-chen.github.io/blog_posts/data%20analysis/markdown/2020/08/26/Freight_Transport.html",
            "relUrl": "/data%20analysis/markdown/2020/08/26/Freight_Transport.html",
            "date": " • Aug 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jinchao-chen.github.io/blog_posts/markdown/2020/01/14/test-markdown-post-hided.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post-hided.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Jinchao Chen . :cn: Chinese on an adventure in Europe :man_mechanic: An engineer who is better at coding than drafting :surfing_man: A thalassophile in addiction with ocean and waves .",
          "url": "https://jinchao-chen.github.io/blog_posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jinchao-chen.github.io/blog_posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}